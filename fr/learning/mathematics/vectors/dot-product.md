# Produit scalaire (`dot product`)

Le **produit scalaire** est une opÃ©ration entre deux vecteurs qui associe un **nombre** Ã  leur relation. Câ€™est une faÃ§on de mesurer **dans quelle mesure deux vecteurs pointent dans la mÃªme direction**.

- Si les vecteurs pointent **dans la mÃªme direction**, le produit scalaire est **grand et positif**.  
- Sâ€™ils sont **orthogonaux (90Â°)**, le produit scalaire vaut **0**.  
- Sâ€™ils pointent **dans des directions opposÃ©es**, il est **nÃ©gatif**.

<img src="/learning/mathematics/vectors/dot-product.png" alt="vecteur" style="display: block; margin: 0 auto;width: 400px; height: auto;">

:::tip ğŸ‘‰ Intuition
Le produit scalaire mesure **lâ€™alignement** entre deux vecteurs, clÃ© en ML pour la similaritÃ© cosinus dans les systÃ¨mes de recommandation ou la recherche sÃ©mantique.
:::

## DÃ©finition mathÃ©matique

Le **produit scalaire** de deux vecteurs en dimension $n$ $\vec{r} = (r_1, r_2, \dots, r_n)$ et $\vec{s} = (s_1, s_2, \dots, s_n)$ est :

$$\vec{r} \cdot \vec{s} = r_1 s_1 + r_2 s_2 + \dots + r_n s_n$$

<img src="/learning/mathematics/vectors/dot-product-2d.png" alt="vecteur" style="display: block; margin: 0 auto;width: 400px; height: auto;">

:::tip ğŸ“Œ
Ce rÃ©sultat est **un nombre rÃ©el** (un *scalaire*), contrairement Ã  lâ€™addition de vecteurs qui donne un vecteur.
:::

:::info Exemple en 2D
$$\vec{r} = \begin{bmatrix} 3 \\ -1 \end{bmatrix}, \quad \vec{s} = \begin{bmatrix} 2 \\ 2 \end{bmatrix}$$

$$\vec{r} \cdot \vec{s} = 3 \times 2 + (-1) \times 2 = 6 - 2 = 4$$
:::

:::details DÃ©veloppement de la formule gÃ©omÃ©trique
Une autre dÃ©finition relie le produit scalaire Ã  lâ€™angle entre deux vecteurs :

$$\vec{r} \cdot \vec{s} = \|\vec{r}\| \, \|\vec{s}\| \cos(\theta)$$

- $\|\vec{r}\|$ = longueur du vecteur $\vec{r}$.
- $\|\vec{s}\|$ = longueur du vecteur $\vec{s}$.
- $\theta$ = angle entre $\vec{r}$ et $\vec{s}$.

Le produit scalaire est grand si les vecteurs sont proches en direction, nul sâ€™ils sont perpendiculaires, et nÃ©gatif sâ€™ils pointent en sens opposÃ©s.
:::

## DÃ©rivation Ã  partir de la rÃ¨gle du cosinus

En gÃ©omÃ©trie, pour un triangle avec cÃ´tÃ©s $a$, $b$, $c$ et angle $\theta$ entre $a$ et $b$ :

$$c^2 = a^2 + b^2 - 2ab \cos(\theta)$$

Câ€™est la **rÃ¨gle du cosinus**, que lâ€™on peut traduire en langage vectoriel.

Soit deux vecteurs $\vec{r}$ et $\vec{s}$. Le troisiÃ¨me cÃ´tÃ© du triangle est donnÃ© par :

$$\vec{r} - \vec{s}$$

La norme au carrÃ© de ce vecteur est donc :

$$\|\vec{r} - \vec{s}\|^2$$

Dâ€™aprÃ¨s la rÃ¨gle du cosinus :

$$\|\vec{r}\|^2 + \|\vec{s}\|^2 - 2 \|\vec{r}\| \|\vec{s}\| \cos(\theta)$$

On peut aussi dÃ©velopper directement avec le **produit scalaire** :

$$\|\vec{r} - \vec{s}\|^2 = (\vec{r} - \vec{s}) \cdot (\vec{r} - \vec{s})$$

DÃ©veloppons :

$$= \vec{r}\cdot\vec{r} - 2 \vec{r}\cdot\vec{s} + \vec{s}\cdot\vec{s}$$

Or :

- $\vec{r}\cdot\vec{r} = \|\vec{r}\|^2$.  
- $\vec{s}\cdot\vec{s} = \|\vec{s}\|^2$.

Donc :

$$\|\vec{r} - \vec{s}\|^2 = \|\vec{r}\|^2 + \|\vec{s}\|^2 - 2 (\vec{r}\cdot\vec{s})$$

En comparant avec la **rÃ¨gle du cosinus** :

$$\|\vec{r} - \vec{s}\|^2 = \|\vec{r}\|^2 + \|\vec{s}\|^2 - 2 \|\vec{r}\| \|\vec{s}\| \cos(\theta)$$

On en dÃ©duit la relation fondamentale :

$$\vec{r} \cdot \vec{s} = \|\vec{r}\| \|\vec{s}\| \cos(\theta)$$

:::details InterprÃ©tation gÃ©omÃ©trique
Le produit scalaire mesure **lâ€™alignement** entre deux vecteurs.

- Si $\theta = 0^\circ$ (mÃªme direction) :  
  $$\cos(0) = 1 \quad \Rightarrow \quad \vec{r}\cdot\vec{s} = \|\vec{r}\|\|\vec{s}\|$$  
  Produit scalaire **positif et maximal**.

- Si $\theta = 90^\circ$ (vecteurs orthogonaux) :  
  $$\cos(90) = 0 \quad \Rightarrow \quad \vec{r}\cdot\vec{s} = 0$$  
  Vecteurs indÃ©pendants.

- Si $\theta = 180^\circ$ (directions opposÃ©es) :  
  $$\cos(180) = -1 \quad \Rightarrow \quad \vec{r}\cdot\vec{s} = -\|\vec{r}\|\|\vec{s}\|$$  
  Produit scalaire **nÃ©gatif**.
:::

:::info RÃ©sumÃ© visuel
- Produit scalaire > 0 : vecteurs dans la **mÃªme direction**.  
- Produit scalaire = 0 : vecteurs **orthogonaux** (indÃ©pendants).  
- Produit scalaire < 0 : vecteurs dans des **directions opposÃ©es**.
:::

## PropriÃ©tÃ©s fondamentales

Voici les principales propriÃ©tÃ©s du produit scalaire :

| PropriÃ©tÃ© | Formulation | InterprÃ©tation |
|-----------|-------------|----------------|
| **CommutativitÃ©** | $\vec{r} \cdot \vec{s} = \vec{s} \cdot \vec{r}$ | Lâ€™ordre ne change rien. |
| **LinÃ©aritÃ© (distributivitÃ©)** | $\vec{r} \cdot (\vec{s} + \vec{t}) = \vec{r} \cdot \vec{s} + \vec{r} \cdot \vec{t}$ | On peut â€œdistribuerâ€ le produit scalaire sur une somme. |
| **HomogÃ©nÃ©itÃ© (associativitÃ© scalaire)** | $(a \vec{r}) \cdot \vec{s} = a (\vec{r} \cdot \vec{s})$ | Un facteur scalaire sort du produit. |
| **Norme** | $\vec{r} \cdot \vec{r} = \|\vec{r}\|^2$ | Le produit scalaire dâ€™un vecteur par lui-mÃªme donne le **carrÃ© de sa longueur**. |

:::info
En ML, ces propriÃ©tÃ©s sont essentielles pour les calculs efficaces dans les rÃ©seaux de neurones, oÃ¹ le produit scalaire apparaÃ®t dans les couches fully-connected.
:::


## Pourquoi câ€™est important en Machine Learning ?

- La **norme** dâ€™un vecteur mesure la taille (ou importance) dâ€™un paramÃ¨tre, dâ€™un poids ou dâ€™une donnÃ©e, comme dans la rÃ©gularisation L2 pour Ã©viter le surapprentissage.  
- Le **produit scalaire** permet de mesurer la **similaritÃ©** entre deux vecteurs, base de la similaritÃ© cosinus :

$$\cos(\theta) = \frac{\vec{r} \cdot \vec{s}}{\|\vec{r}\| \|\vec{s}\|}$$

En IA, câ€™est central pour :

- Les systÃ¨mes de recommandation (e.g., Netflix utilise la similaritÃ© cosinus sur les embeddings d'utilisateurs et de films).  
- La recherche sÃ©mantique (e.g., dans les moteurs comme Elasticsearch avec embeddings BERT).  
- Le clustering (e.g., k-means utilise des distances basÃ©es sur normes).  
- Les transformers (attention mechanism utilise des produits scalaires pour peser les relations entre tokens).

:::info Exemple concret en ML
En NLP, deux embeddings de mots (e.g., "roi" et "reine") ont un produit scalaire Ã©levÃ© si similaires, permettant des analogies comme "roi - homme + femme â‰ˆ reine".  
En data science, la similaritÃ© cosinus compare des documents ou des images pour la dÃ©tection de similaritÃ©s.
:::