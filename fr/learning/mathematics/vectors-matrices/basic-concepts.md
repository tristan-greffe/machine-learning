# Notions de base

## Qu‚Äôest-ce qu‚Äôun vecteur ?

Un **vecteur** est un objet math√©matique qui poss√®de deux propri√©t√©s fondamentales :

- Une **direction** (l‚Äôorientation dans l‚Äôespace).  
- Une **norme** (sa longueur, ou taille).

:::info
En machine learning, les vecteurs repr√©sentent souvent des donn√©es comme des embeddings, o√π la direction encode des similarit√©s s√©mantiques (par exemple, dans les mod√®les de langage comme Word2Vec ou BERT), et la norme peut refl√©ter l'importance ou la fr√©quence d'une caract√©ristique.
:::

## Repr√©sentation dans un syst√®me de coordonn√©es

En 2D, on choisit souvent deux vecteurs unitaires orthogonaux $\vec{i}$, $\vec{j}$ :

<img src="/learning/mathematics/vectors-matrices/coordinate-system.jpg" alt="vecteur" style="display: block; margin: 0 auto;width: 400px; height: auto;">

$$\vec{r} = a \cdot \vec{i} + b \cdot \vec{j}$$

- $a$ : composante sur l‚Äôaxe $x$.  
- $b$ : composante sur l‚Äôaxe $y$.  

En pratique, on √©crit souvent le vecteur sous forme de **colonne** :  

$$\vec{r} = \begin{bmatrix} a \\ b \end{bmatrix}$$

:::tip üëâ Extension en dimensions sup√©rieures
En $n$ dimensions, un vecteur est $\vec{r} = (r_1, r_2, \dots, r_n)$, courant en ML pour les embeddings de haute dimension (e.g., 768 pour BERT).
:::

:::info
En data science, cette repr√©sentation est utilis√©e pour les features dans les datasets, comme dans les algorithmes de clustering (e.g., k-means) o√π les points sont des vecteurs dans un espace multidimensionnel.
:::

## Norme (longueur) d‚Äôun vecteur

La **longueur** (ou norme) d‚Äôun vecteur se calcule avec le th√©or√®me de Pythagore :

$$\|\vec{r}\| = \sqrt{a^2 + b^2}$$

<img src="/learning/mathematics/vectors-matrices/vector-norm.png" alt="Norme d'un vecteur avec th√©or√®me de Pythagore" style="display: block; margin: 0 auto; width: 500px; height: auto;">

:::tip üëâ En dimensions sup√©rieures
$$\|\vec{r}\| = \sqrt{r_1^2 + r_2^2 + \dots + r_n^2}$$
En ML, la norme L2 (euclidienne) mesure la "taille" d'un vecteur, utile pour la r√©gularisation (e.g., Ridge regression) ou la normalisation des embeddings.
:::

:::info Exemple en 2D
$$\vec{r} = \begin{bmatrix} 3 \\ 4 \end{bmatrix}$$

$$\|\vec{r}\| = \sqrt{3^2 + 4^2} = 5$$
:::