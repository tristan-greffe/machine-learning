# Base Vectorielle

## Qu‚Äôest-ce qu‚Äôune base ?

Une **base** est un ensemble de vecteurs qui permet de d√©crire **tous les vecteurs** d‚Äôun espace vectoriel en utilisant des **combinaisons lin√©aires** de ces vecteurs. En d'autres termes, une base est comme un "syst√®me de coordonn√©es" pour un espace, permettant de repr√©senter n'importe quel point ou vecteur. Une base fournit un ensemble minimal de directions ind√©pendantes pour naviguer dans l'espace.

<img src="/learning/mathematics/vectors-matrices/vector-space-basis.png" alt="Illustration d'une base vectorielle" style="display: block; margin: 0 auto; width: 300px; height: auto;">

:::tip üëâ Intuition
Une base est comme un ensemble minimal de "directions" qui permet de naviguer dans tout l‚Äôespace. En ML, changer de base peut simplifier les calculs, comme dans la r√©duction de dimensionnalit√© o√π l'on projette les donn√©es sur une base qui capture l'essentiel de l'information.
:::

:::info
En machine learning, les bases sont essentielles pour comprendre les transformations de donn√©es, comme dans l'**analyse en composantes principales (PCA)** o√π une nouvelle base est choisie pour maximiser la variance des donn√©es, ou dans les r√©seaux de neurones o√π les couches apprennent des bases qui extraient des caract√©ristiques pertinentes (features) des donn√©es d'entr√©e.
:::

### D√©finition math√©matique
Un ensemble de vecteurs $\{\vec{b_1}, \vec{b_2}, \dots, \vec{b_n}\}$ forme une **base** d‚Äôun espace vectoriel si :
1. Les vecteurs sont **lin√©airement ind√©pendants** :
   $$a_1 \vec{b_1} + a_2 \vec{b_2} + \dots + a_n \vec{b_n} = \vec{0} \quad \Rightarrow \quad a_1 = a_2 = \dots = a_n = 0$$
2. Les vecteurs **engendrent l‚Äôespace** (ou le spannent), c‚Äôest-√†-dire que tout vecteur $\vec{v}$ de l‚Äôespace peut s‚Äô√©crire comme :
   $$\vec{v} = c_1 \vec{b_1} + c_2 \vec{b_2} + \dots + c_n \vec{b_n}$$

Le nombre $n$ de vecteurs dans la base correspond √† la **dimension** de l‚Äôespace.

:::details D√©veloppement : Dimension et bases multiples
Tous les espaces vectoriels de dimension finie ont plusieurs bases possibles, mais toutes les bases ont le m√™me nombre d'√©l√©ments (th√©or√®me de la dimension). Par exemple, en $\mathbb{R}^2$, la base standard $\{\vec{i}, \vec{j}\}$ et une base rot√©e sont toutes deux valides, mais la dimension reste 2. En ML, cela permet de choisir une base qui aligne les donn√©es pour mieux r√©v√©ler des patterns, comme dans PCA o√π les vecteurs propres forment une base qui d√©corr√®le les variables.
:::

:::info Exemples en dimensions basses
- En **1D** (une ligne) : Un seul vecteur non nul $\vec{b_1}$ suffit. Tout vecteur sur la ligne est un multiple de $\vec{b_1}$.
- En **2D** (un plan) : Deux vecteurs ind√©pendants $\vec{b_1}, \vec{b_2}$. Tout vecteur dans le plan s'√©crit $a_1 \vec{b_1} + a_2 \vec{b_2}$.
- En **3D** : Trois vecteurs ind√©pendants $\vec{b_1}, \vec{b_2}, \vec{b_3}$. Si on ajoute un $\vec{b_4}$ non dans le plan des trois premiers, on passe √† 4D.
En ML, les donn√©es en haute dimension (e.g., images avec des milliers de pixels) sont souvent projet√©es sur une base de plus faible dimension pour r√©duire le bruit et la complexit√©.
:::

## Ind√©pendance lin√©aire

Un ensemble de vecteurs est **lin√©airement ind√©pendant** si aucun des vecteurs ne peut √™tre exprim√© comme une combinaison lin√©aire des autres. Autrement dit, aucun vecteur n‚Äôest "redondant". Si un vecteur est une combinaison des autres, il n'apporte pas de nouvelle direction.

:::info Exemple
- Si $\vec{b_2} = 2 \vec{b_1}$, alors $\vec{b_2}$ est dans la m√™me direction que $\vec{b_1}$ et n‚Äôapporte rien de nouveau ‚Üí **d√©pendance lin√©aire**.
- Si $\vec{b_2}$ n‚Äôest pas un multiple de $\vec{b_1}$ (par exemple, ils forment un angle non nul), alors ils sont **ind√©pendants** et peuvent engendrer un plan (espace 2D).
En ML, l'ind√©pendance lin√©aire aide √† d√©tecter les features redondantes dans un dataset (e.g., via corr√©lation ou PCA).
:::

:::details V√©rification de l‚Äôind√©pendance lin√©aire
Pour v√©rifier si des vecteurs sont lin√©airement ind√©pendants, on peut :
1. Former une matrice avec ces vecteurs comme colonnes.
2. Calculer son **d√©terminant** (non nul = ind√©pendance) ou v√©rifier son **rang** (√©gal au nombre de vecteurs = ind√©pendance).
   - En 2D, pour $\vec{b_1} = \begin{bmatrix} x_1 \\ y_1 \end{bmatrix}$, $\vec{b_2} = \begin{bmatrix} x_2 \\ y_2 \end{bmatrix}$, le d√©terminant de la matrice $\begin{bmatrix} x_1 & x_2 \\ y_1 & y_2 \end{bmatrix}$ est $x_1 y_2 - x_2 y_1$. S‚Äôil est non nul, les vecteurs sont ind√©pendants.
En data science, un rang inf√©rieur indique des d√©pendances, utile pour la s√©lection de features.
:::

:::details Pourquoi le d√©terminant v√©rifie-t-il l‚Äôind√©pendance lin√©aire ?
Le d√©terminant d‚Äôune matrice carr√©e form√©e par des vecteurs est un outil puissant pour v√©rifier leur ind√©pendance lin√©aire. Voici pourquoi, √©tape par √©tape :

### D√©finition et lien avec l‚Äôind√©pendance lin√©aire
Un ensemble de $n$ vecteurs $\{\vec{v_1}, \vec{v_2}, \dots, \vec{v_n}\}$ dans $\mathbb{R}^n$ est **lin√©airement ind√©pendant** si l‚Äô√©quation suivante n‚Äôa que la solution triviale (tous les coefficients nuls) :
$$c_1 \vec{v_1} + c_2 \vec{v_2} + \dots + c_n \vec{v_n} = \vec{0} \quad \Rightarrow \quad c_1 = c_2 = \dots = c_n = 0.$$

Formons la matrice $A = [\vec{v_1} \, \vec{v_2} \, \dots \, \vec{v_n}]$, o√π chaque $\vec{v_i}$ est une colonne. L‚Äô√©quation ci-dessus s‚Äô√©crit sous forme matricielle :
$$A \vec{c} = \vec{0},$$
o√π $\vec{c} = \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{bmatrix}$. Les vecteurs sont ind√©pendants si la seule solution est $\vec{c} = \vec{0}$.

### R√¥le du d√©terminant
Si $A$ est une matrice carr√©e ($n \times n$), son d√©terminant $\det(A)$ indique si $A$ est **inversible** :
- **Si $\det(A) \neq 0$** : $A$ est inversible, et le syst√®me $A \vec{c} = \vec{0}$ n‚Äôa que la solution triviale $\vec{c} = \vec{0}$. Les vecteurs sont donc **ind√©pendants**.
- **Si $\det(A) = 0$** : $A$ n‚Äôest pas inversible, et le syst√®me a des solutions non triviales (des $\vec{c} \neq \vec{0}$). Cela signifie qu‚Äôau moins un vecteur est une combinaison lin√©aire des autres, donc les vecteurs sont **d√©pendants**.

### Interpr√©tation g√©om√©trique
Le d√©terminant a une signification g√©om√©trique :
- En 2D, pour deux vecteurs, $\det(A)$ repr√©sente l‚Äôaire du parall√©logramme form√© par ces vecteurs. Si $\det(A) = 0$, l‚Äôaire est nulle, ce qui signifie que les vecteurs sont align√©s (colin√©aires), donc d√©pendants.
- En 3D, pour trois vecteurs, $\det(A)$ repr√©sente le volume du parall√©l√©pip√®de. Si $\det(A) = 0$, le volume est nul, ce qui indique que les vecteurs sont coplanaires (ou align√©s), donc d√©pendants.
En g√©n√©ral, un d√©terminant nul signifie que les vecteurs ne couvrent pas tout l‚Äôespace $\mathbb{R}^n$, mais un sous-espace de dimension inf√©rieure.

### Application √† l‚Äôexemple du cours
Reprenons les vecteurs $\vec{a} = \begin{bmatrix} 1 \\ 2 \\ -1 \end{bmatrix}$, $\vec{b} = \begin{bmatrix} 3 \\ -4 \\ 5 \end{bmatrix}$, $\vec{c} = \begin{bmatrix} 1 \\ -8 \\ 7 \end{bmatrix}$. Formons la matrice :
$$A = \begin{bmatrix} 1 & 3 & 1 \\ 2 & -4 & -8 \\ -1 & 5 & 7 \end{bmatrix}$$

Calculons le d√©terminant :
$$\det(A) = 1 \cdot \det\begin{bmatrix} -4 & -8 \\ 5 & 7 \end{bmatrix} - 3 \cdot \det\begin{bmatrix} 2 & -8 \\ -1 & 7 \end{bmatrix} + 1 \cdot \det\begin{bmatrix} 2 & -4 \\ -1 & 5 \end{bmatrix}$$

- Premier mineur : $(-4)(7) - (-8)(5) = -28 + 40 = 12$
- Deuxi√®me mineur : $(2)(7) - (-8)(-1) = 14 - 8 = 6$
- Troisi√®me mineur : $(2)(5) - (-4)(-1) = 10 - 4 = 6$

Ainsi :
$$\det(A) = 1 \cdot 12 - 3 \cdot 6 + 1 \cdot 6 = 12 - 18 + 6 = 0$$

Puisque $\det(A) = 0$, les vecteurs sont **d√©pendants**. G√©om√©triquement, ils sont coplanaires dans $\mathbb{R}^3$, ne formant pas une base.

### Cas non carr√©
Si le nombre de vecteurs $n$ diff√®re de la dimension $m$ (par exemple, 4 vecteurs dans $\mathbb{R}^3$), la matrice n‚Äôest pas carr√©e, et le d√©terminant n‚Äôest pas d√©fini. Dans ce cas, on utilise le **rang** de la matrice (via l‚Äô√©limination de Gauss) pour v√©rifier l‚Äôind√©pendance.

### En machine learning
Le d√©terminant est utilis√© pour d√©tecter les redondances dans les features (si $\det = 0$, il y a corr√©lation). Dans PCA, un d√©terminant non nul des vecteurs propres garantit une base valide pour la projection des donn√©es.

### R√©sum√©
Le d√©terminant teste l‚Äôind√©pendance lin√©aire car il mesure si les vecteurs couvrent pleinement l‚Äôespace. S‚Äôil est nul, les vecteurs sont confin√©s √† un sous-espace, donc d√©pendants. S‚Äôil est non nul, ils forment une base.
:::

:::info Exemple en 2D
Consid√©rons :
- $\vec{b_1} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$, $\vec{b_2} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$ (base canonique 2D).
- Ces vecteurs sont lin√©airement ind√©pendants, car $a_1 \begin{bmatrix} 1 \\ 0 \end{bmatrix} + a_2 \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} a_1 \\ a_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$ implique $a_1 = a_2 = 0$.
- Tout vecteur $\vec{v} = \begin{bmatrix} x \\ y \end{bmatrix}$ peut s‚Äô√©crire : $\vec{v} = x \vec{b_1} + y \vec{b_2}$.
:::

:::details Exemple suppl√©mentaire : V√©rification d'ind√©pendance lin√©aire en 3D
Les vecteurs suivants sont-ils lin√©airement ind√©pendants ? 

$\vec{a} = \begin{bmatrix} 1 \\ 2 \\ -1 \end{bmatrix}$, $\vec{b} = \begin{bmatrix} 3 \\ -4 \\ 5 \end{bmatrix}$ et $\vec{c} = \begin{bmatrix} 1 \\ -8 \\ 7 \end{bmatrix}$.

Pour v√©rifier, on forme la matrice $M$ avec ces vecteurs comme colonnes :

$$M = \begin{bmatrix} 1 & 3 & 1 \\ 2 & -4 & -8 \\ -1 & 5 & 7 \end{bmatrix}$$

On calcule le d√©terminant de $M$. Si $\det(M) \neq 0$, les vecteurs sont ind√©pendants ; sinon, ils sont d√©pendants.

Calcul du d√©terminant √©tape par √©tape (utilisant la formule pour une matrice 3x3) :

$$\det(M) = 1 \cdot \det\begin{bmatrix} -4 & -8 \\ 5 & 7 \end{bmatrix} - 3 \cdot \det\begin{bmatrix} 2 & -8 \\ -1 & 7 \end{bmatrix} + 1 \cdot \det\begin{bmatrix} 2 & -4 \\ -1 & 5 \end{bmatrix}$$

- Premier mineur : $\det\begin{bmatrix} -4 & -8 \\ 5 & 7 \end{bmatrix} = (-4)(7) - (-8)(5) = -28 + 40 = 12$
- Deuxi√®me mineur : $\det\begin{bmatrix} 2 & -8 \\ -1 & 7 \end{bmatrix} = (2)(7) - (-8)(-1) = 14 - 8 = 6$
- Troisi√®me mineur : $\det\begin{bmatrix} 2 & -4 \\ -1 & 5 \end{bmatrix} = (2)(5) - (-4)(-1) = 10 - 4 = 6$

Maintenant, assembler :

$$\det(M) = 1 \cdot 12 - 3 \cdot 6 + 1 \cdot 6 = 12 - 18 + 6 = 0$$

Puisque $\det(M) = 0$, les vecteurs sont lin√©airement **d√©pendants**. Cela signifie qu'au moins un vecteur peut s'exprimer comme une combinaison lin√©aire des autres (par exemple, on peut r√©soudre pour trouver des coefficients non triviaux tels que $\vec{c} = -2\vec{a} + \vec{b}$, mais la v√©rification du d√©terminant suffit pour conclure).
:::

## Engendrement de l'espace (Span)

Un ensemble de vecteurs **engendre** l'espace s'il permet d'atteindre tout vecteur via des combinaisons lin√©aires. La base est l'ensemble minimal (ind√©pendant) qui engendre l'espace.

:::details Interpr√©tation
Si les vecteurs n'engendrent pas l'espace, ils ne couvrent qu'un sous-espace (e.g., deux vecteurs align√©s engendrent une ligne, pas un plan). En ML, le span des donn√©es peut r√©v√©ler des sous-espaces de haute variance, comme dans PCA o√π l'on identifie les directions principales.
:::

:::info Exemple en ML : R√©duction de dimensionnalit√©
Supposez des points de donn√©es align√©s approximativement sur une ligne dans $\mathbb{R}^2$. La base optimale serait un vecteur le long de cette ligne (direction de variance maximale) et un perpendiculaire (bruit). Projeter sur la premi√®re direction r√©duit √† 1D tout en conservant l'information principale, comme dans PCA.
:::

<img src="/learning/mathematics/vectors-matrices/pca-fish.png" alt="Illustration de PCA : projection sur axes principaux" style="display: block; margin: 0 auto; width: 500px; height: auto;">

## Bases sp√©ciales

### Base orthonorm√©e
Une base est **orthonorm√©e** si :
- Les vecteurs sont **orthogonaux** deux √† deux : $\vec{b_i} \cdot \vec{b_j} = 0$ si $i \neq j$.
- Les vecteurs sont **unitaires** : $\|\vec{b_i}\| = 1$.

<img src="/learning/mathematics/vectors-matrices/orthogonal-basis.png" alt="Base orthonorm√©e" style="display: block; margin: 0 auto; width: 500px; height: auto;">

:::tip üëâ Avantage
Les bases orthonorm√©es simplifient les calculs en ML, notamment dans les projections (comme dans PCA) ou les transformations (e.g., matrices de rotation dans les r√©seaux de neurones). Elles ne doivent pas n√©cessairement √™tre unitaires ou orthogonales, mais c'est plus facile si elles le sont.
:::

:::info Exemple de base orthonorm√©e
La base canonique en 2D :
- $\vec{b_1} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$, $\vec{b_2} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$.
- V√©rification :
  - Orthogonalit√© : $\vec{b_1} \cdot \vec{b_2} = 1 \cdot 0 + 0 \cdot 1 = 0$.
  - Unitaire : $\|\vec{b_1}\| = \sqrt{1^2 + 0^2} = 1$, $\|\vec{b_2}\| = \sqrt{0^2 + 1^2} = 1$.
:::

### Base quelconque
Une base quelconque n‚Äôest pas n√©cessairement orthonorm√©e. Les vecteurs peuvent avoir des longueurs diff√©rentes et ne pas √™tre orthogonaux.

:::tip üëâ Note
Les bases non orthonorm√©es compliquent les calculs, car les projections n√©cessitent des matrices de changement de base. En ML, on pr√©f√®re souvent orthonormer les bases (e.g., via la d√©composition QR ou SVD) pour simplifier les op√©rations.
:::

## Changement de base

Le **changement de base** consiste √† r√©√©crire un vecteur exprim√© dans une base $\{\vec{b_1}, \vec{b_2}, \dots, \vec{b_n}\}$ dans une autre base $\{\vec{c_1}, \vec{c_2}, \dots, \vec{c_n}\}$. Cela revient √† transformer les coordonn√©es du vecteur, en pr√©servant les propri√©t√©s lin√©aires : l'espace reste une grille uniform√©ment espac√©e, sans courbure.

<img src="/learning/mathematics/vectors-matrices/vector-base-change.gif" alt="changement de base vectorielle" style="display: block; margin: 0 auto; width: 500px; height: auto;">

### Principe
Soit un vecteur $\vec{v}$ exprim√© dans une base $B = \{\vec{b_1}, \vec{b_2}\}$ :
$$\vec{v} = x_1 \vec{b_1} + x_2 \vec{b_2}$$
On veut ses coordonn√©es dans une nouvelle base $C = \{\vec{c_1}, \vec{c_2}\}$ :
$$\vec{v} = y_1 \vec{c_1} + y_2 \vec{c_2}$$

La transformation est effectu√©e √† l‚Äôaide d‚Äôune **matrice de changement de base** $P$, o√π les colonnes de $P$ sont les vecteurs de la base $C$ exprim√©s dans la base $B$. Les nouvelles coordonn√©es sont :
$$\begin{bmatrix} y_1 \\ y_2 \end{bmatrix} = P^{-1} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}$$

Si les bases ne sont pas orthogonales, on ne peut pas utiliser seulement le produit scalaire ; il faut des matrices.

:::details D√©veloppement math√©matique
Si $\vec{c_1} = p_{11} \vec{b_1} + p_{21} \vec{b_2}$ et $\vec{c_2} = p_{12} \vec{b_1} + p_{22} \vec{b_2}$, la matrice $P$ est :
$$P = \begin{bmatrix} p_{11} & p_{12} \\ p_{21} & p_{22} \end{bmatrix}$$
Alors, pour passer de la base $B$ √† $C$, on utilise $P^{-1}$. En ML, les changements de base sont utilis√©s dans les transformations lin√©aires (e.g., rotation des donn√©es dans PCA ou embeddings dans les transformers).
:::

:::info Exemple en 2D
Soit une base $B = \left\{ \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right\}$ et une base $C = \left\{ \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \begin{bmatrix} -1 \\ 1 \end{bmatrix} \right\}$. Un vecteur $\vec{v} = \begin{bmatrix} 3 \\ 2 \end{bmatrix}$ dans $B$ a pour coordonn√©es :
- Dans $B$ : $\vec{v} = 3 \vec{b_1} + 2 \vec{b_2}$.
- Pour trouver les coordonn√©es dans $C$, on r√©sout $\vec{v} = y_1 \vec{c_1} + y_2 \vec{c_2}$ :
  $$3 \begin{bmatrix} 1 \\ 0 \end{bmatrix} + 2 \begin{bmatrix} 0 \\ 1 \end{bmatrix} = y_1 \begin{bmatrix} 1 \\ 1 \end{bmatrix} + y_2 \begin{bmatrix} -1 \\ 1 \end{bmatrix}$$
  Cela donne le syst√®me :
  - $y_1 - y_2 = 3$
  - $y_1 + y_2 = 2$
  Solution : $y_1 = 2.5$, $y_2 = -0.5$. Donc, $\vec{v} = 2.5 \vec{c_1} - 0.5 \vec{c_2}$.
:::

:::details Exemple suppl√©mentaire : Changement de base en 3D avec base orthogonale
√âtant donn√© les vecteurs $\vec{v} = \begin{bmatrix} -4 \\ -3 \\ 8 \end{bmatrix}$, $\vec{b_1} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$, $\vec{b_2} = \begin{bmatrix} -2 \\ 1 \\ 0 \end{bmatrix}$ et $\vec{b_3} = \begin{bmatrix} -3 \\ -6 \\ 5 \end{bmatrix}$, tous √©crits dans la base standard, que repr√©sente $\vec{v}$ dans la base d√©finie par $\vec{b_1}$, $\vec{b_2}$ et $\vec{b_3}$ ? On sait que $\vec{b_1}$, $\vec{b_2}$ et $\vec{b_3}$ sont orthogonaux les uns par rapport aux autres.

Puisque la base $\{\vec{b_1}, \vec{b_2}, \vec{b_3}\}$ est orthogonale (mais pas n√©cessairement orthonorm√©e, car les normes ne sont pas forc√©ment 1), on peut trouver les coordonn√©es $c_1, c_2, c_3$ telles que $\vec{v} = c_1 \vec{b_1} + c_2 \vec{b_2} + c_3 \vec{b_3}$ en utilisant la formule de projection pour bases orthogonales : $c_i = \frac{\vec{v} \cdot \vec{b_i}}{\vec{b_i} \cdot \vec{b_i}}$.

D√©taillons les √©tapes :

1. V√©rifions d'abord l'orthogonalit√© (bien que donn√©, pour compl√©tude) :
   - $\vec{b_1} \cdot \vec{b_2} = 1 \cdot (-2) + 2 \cdot 1 + 3 \cdot 0 = -2 + 2 + 0 = 0$
   - $\vec{b_1} \cdot \vec{b_3} = 1 \cdot (-3) + 2 \cdot (-6) + 3 \cdot 5 = -3 -12 + 15 = 0$
   - $\vec{b_2} \cdot \vec{b_3} = (-2) \cdot (-3) + 1 \cdot (-6) + 0 \cdot 5 = 6 -6 + 0 = 0$
   Oui, orthogonaux.

2. Pour trouver $c_1$ : On multiplie l'√©quation $\vec{v} = c_1 \vec{b_1} + c_2 \vec{b_2} + c_3 \vec{b_3}$ par $\vec{b_1}$ (produit scalaire des deux c√¥t√©s) :
   $$\vec{v} \cdot \vec{b_1} = c_1 (\vec{b_1} \cdot \vec{b_1}) + c_2 (\vec{b_2} \cdot \vec{b_1}) + c_3 (\vec{b_3} \cdot \vec{b_1})$$
   Puisque orthogonaux, $\vec{b_2} \cdot \vec{b_1} = 0$ et $\vec{b_3} \cdot \vec{b_1} = 0$, donc :
   $$\vec{v} \cdot \vec{b_1} = c_1 (\vec{b_1} \cdot \vec{b_1})$$
   $$c_1 = \frac{\vec{v} \cdot \vec{b_1}}{\vec{b_1} \cdot \vec{b_1}}$$
   Calcul : $\vec{v} \cdot \vec{b_1} = (-4)(1) + (-3)(2) + 8(3) = -4 -6 + 24 = 14$
   $\vec{b_1} \cdot \vec{b_1} = 1^2 + 2^2 + 3^2 = 1 + 4 + 9 = 14$
   $c_1 = 14 / 14 = 1$

3. De m√™me pour $c_2$ : Multipliez par $\vec{b_2}$ :
   $$\vec{v} \cdot \vec{b_2} = c_2 (\vec{b_2} \cdot \vec{b_2})$$ (termes crois√©s nuls)
   $c_2 = \frac{\vec{v} \cdot \vec{b_2}}{\vec{b_2} \cdot \vec{b_2}}$
   $\vec{v} \cdot \vec{b_2} = (-4)(-2) + (-3)(1) + 8(0) = 8 -3 + 0 = 5$
   $\vec{b_2} \cdot \vec{b_2} = (-2)^2 + 1^2 + 0^2 = 4 + 1 + 0 = 5$
   $c_2 = 5 / 5 = 1$

4. Pour $c_3$ : Multipliez par $\vec{b_3}$ :
   $$\vec{v} \cdot \vec{b_3} = c_3 (\vec{b_3} \cdot \vec{b_3})$$
   $c_3 = \frac{\vec{v} \cdot \vec{b_3}}{\vec{b_3} \cdot \vec{b_3}}$
   $\vec{v} \cdot \vec{b_3} = (-4)(-3) + (-3)(-6) + 8(5) = 12 + 18 + 40 = 70$
   $\vec{b_3} \cdot \vec{b_3} = (-3)^2 + (-6)^2 + 5^2 = 9 + 36 + 25 = 70$
   $c_3 = 70 / 70 = 1$

Donc, dans la base $\{\vec{b_1}, \vec{b_2}, \vec{b_3}\}$, $\vec{v}$ s'√©crit comme $\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$, c'est-√†-dire $\vec{v} = 1 \cdot \vec{b_1} + 1 \cdot \vec{b_2} + 1 \cdot \vec{b_3}$.
:::

## Pourquoi c‚Äôest important en Machine Learning ?

- **Repr√©sentation des donn√©es** : Les bases permettent de repr√©senter les donn√©es dans des espaces de caract√©ristiques. En PCA, les composantes principales forment une base orthonorm√©e qui maximise la variance des donn√©es et minimise le bruit (distance perpendiculaire comme mesure du bruit).
- **Transformations lin√©aires** : Les changements de base sont utilis√©s pour simplifier les calculs, comme dans les r√©seaux de neurones o√π les poids sont des matrices appliquant des transformations lin√©aires pour extraire des features (e.g., forme du nez, teinte de peau dans la reconnaissance faciale).
- **R√©duction de dimensionnalit√©** : En ML, on utilise des bases orthonorm√©es (via SVD ou QR) pour r√©duire la dimensionnalit√© tout en pr√©servant les informations importantes (e.g., embeddings dans NLP). Si les donn√©es sont align√©es sur une ligne, projeter sur cette direction r√©duit le bruit.
- **M√©canisme d‚Äôattention** : Dans les transformers, les matrices de changement de base (ou projections lin√©aires) sont utilis√©es pour calculer les relations entre les tokens via des produits scalaires.

:::info Exemple concret en ML
Dans **PCA**, les donn√©es sont projet√©es sur une base orthonorm√©e form√©e par les vecteurs propres de la matrice de covariance. Cela r√©duit la dimensionnalit√© tout en conservant la variance maximale, facilitant la visualisation ou la classification. Par exemple, des points align√©s sur une ligne ont une variance √©lev√©e le long de la ligne et faible perpendiculairement (bruit).
:::

:::tip üëâ Application pratique
En **NLP**, les embeddings de mots (comme dans Word2Vec) sont souvent exprim√©s dans une base non orthonorm√©e. Les algorithmes comme **t-SNE** ou **UMAP** changent de base pour visualiser ces donn√©es en 2D ou 3D. Dans les r√©seaux de neurones, l'apprentissage d√©rive une base qui extrait les caract√©ristiques les plus riches des donn√©es.
:::